{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8eaee0",
   "metadata": {},
   "source": [
    "# Image classification with transformers\n",
    "A simple transformer based image classification workflow which classifies \n",
    "terrain from the 'Intel image classification dataset'.\n",
    "\n",
    "The whole model is not trained rather a classification layer is added on top and finetuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc941d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torchvision\n",
    "# from torchvision import datasets\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d6425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm \n",
    "# LabelSmoothingCrossEntropy provides better results\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa43b161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39831821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27d277a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test dir: \n",
      " ../datasets/intel_image_classification/seg_train/seg_train \n",
      " ../datasets/intel_image_classification/seg_test/seg_test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set dataset paths\n",
    "dataset_path = Path('../datasets/intel_image_classification')\n",
    "\n",
    "# Setup train and testing paths\n",
    "train_dir = dataset_path / \"seg_train/seg_train\"\n",
    "test_dir = dataset_path / \"seg_test/seg_test\"\n",
    "\n",
    "print('train test dir: \\n', train_dir, '\\n', test_dir)\n",
    "\n",
    "os.path.exists(train_dir), os.path.exists(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1284e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the transformations for the purpose of data augmentations\n",
    "# train data transform\n",
    "train_data_transform = transforms.Compose([\n",
    "  \n",
    "  # image transformations\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.RandomVerticalFlip(),\n",
    "  transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter()]), p=0.25),\n",
    "  transforms.Resize((224, 224)),\n",
    "\n",
    "  # Turn the image into a torch.Tensor\n",
    "  transforms.ToTensor(),\n",
    "\n",
    "  # normalise\n",
    "  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n",
    "  transforms.RandomErasing(p=0.2, value='random'),\n",
    "   \n",
    "])\n",
    "\n",
    "# test data transform\n",
    "test_data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # normalise with imagenet pretrained values\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e280b0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset ImageFolder\n",
       "     Number of datapoints: 14034\n",
       "     Root location: ../datasets/intel_image_classification/seg_train/seg_train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                RandomVerticalFlip(p=0.5)\n",
       "                RandomApply(\n",
       "                p=0.25\n",
       "                ColorJitter(brightness=None, contrast=None, saturation=None, hue=None)\n",
       "            )\n",
       "                Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "                RandomErasing(p=0.2, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=random, inplace=False)\n",
       "            ),\n",
       " Dataset ImageFolder\n",
       "     Number of datapoints: 3000\n",
       "     Root location: ../datasets/intel_image_classification/seg_test/seg_test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "            ))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use ImageFolder to create standard pytorch dataset(s)\n",
    "from torchvision import datasets\n",
    "train_data = datasets.ImageFolder(root=train_dir,\n",
    "                                  transform=train_data_transform, # a transform for the data\n",
    "                                  target_transform=None) # a transform for the label/target \n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                 transform=test_data_transform)\n",
    "\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7a1e51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_classes, classes:  6 ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n"
     ]
    }
   ],
   "source": [
    "# check classes\n",
    "assert train_data.classes == test_data.classes\n",
    "n_classes = len(train_data.classes)\n",
    "print('n_classes, classes: ', n_classes, train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c26afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training variables\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 1\n",
    "LR = 0.001\n",
    "# scheduler\n",
    "LRS_STEP_SIZE = 3\n",
    "LRS_GAMMA = 0.97\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a23d0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fef201d1fa0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fef201d1be0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the datasets created in last set to create dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=NUM_WORKERS,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             shuffle=False)\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c72e8236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# check loader shape\n",
    "print(next(iter(train_dataloader))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f008c7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloaders, dataset_sizes {'train': <torch.utils.data.dataloader.DataLoader object at 0x7fef201d1fa0>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7fef201d1be0>} {'train': 14034, 'val': 3000}\n"
     ]
    }
   ],
   "source": [
    "# put dataloader into dict to be used in training loop\n",
    "dataloaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"val\": test_dataloader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    \"train\": len(train_data),\n",
    "    \"val\": len(test_data)\n",
    "}\n",
    "\n",
    "print('dataloaders, dataset_sizes', dataloaders, dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b84d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/bappadityadebnath/.cache/torch/hub/facebookresearch_deit_main\n"
     ]
    }
   ],
   "source": [
    "# download a pre-trained transformer model\n",
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faf65dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=192, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.3, inplace=False)\n",
      "  (3): Linear(in_features=512, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# freeze the model and add a classification head\n",
    "for param in model.parameters(): #freeze model\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_inputs = model.head.in_features\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(n_inputs, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, n_classes)\n",
    ")\n",
    "model = model.to(device)\n",
    "print(model.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f0917f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training loss and optimizer\n",
    "criterion = LabelSmoothingCrossEntropy()\n",
    "criterion = criterion.to(device)\n",
    "# optimizer = optim.Adam(model.head.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LRS_STEP_SIZE, gamma=LRS_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c41a6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        # one training and validation phase per epoch\n",
    "        for phase in ['train', 'val']: \n",
    "            if phase == 'train':\n",
    "                # model to training mode\n",
    "                model.train() \n",
    "            else:\n",
    "                # model to evaluate mode\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            for idx, (inputs, labels) in tqdm(enumerate(dataloaders[phase])):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation go faster\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1) # used for accuracy\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # accumulate looses and correct score\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                # print('torch.sum', torch.sum(preds == labels.data), running_corrects, idx, 8 * idx)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                # LRS step at end of epoch\n",
    "                scheduler.step() \n",
    "            \n",
    "            # calculate total looses\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc =  running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # save best model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                # keep the best model\n",
    "                best_model_wts = copy.deepcopy(model.state_dict()) \n",
    "        print()\n",
    "    time_elapsed = time.time() - since # slight error\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    # return best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0368076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1755it [00:19, 91.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6925 Acc: 0.8792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "375it [00:04, 88.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6635 Acc: 0.8830\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1755it [00:19, 88.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6745 Acc: 0.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "375it [00:03, 96.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6386 Acc: 0.9100\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1755it [00:20, 87.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6614 Acc: 0.8961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "375it [00:03, 95.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6230 Acc: 0.9163\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1755it [00:19, 88.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6554 Acc: 0.9016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "375it [00:03, 97.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6243 Acc: 0.9200\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1755it [00:19, 87.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6475 Acc: 0.9048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "375it [00:03, 94.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6236 Acc: 0.9133\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1755it [00:20, 87.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6400 Acc: 0.9118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "375it [00:03, 97.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6221 Acc: 0.9137\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1755it [00:19, 88.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6388 Acc: 0.9111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "375it [00:03, 96.63it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6345 Acc: 0.9100\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1755it [00:19, 88.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6352 Acc: 0.9134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "375it [00:03, 95.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6159 Acc: 0.9200\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1755it [00:20, 87.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6283 Acc: 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "375it [00:03, 95.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6170 Acc: 0.9117\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1755it [00:20, 86.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6260 Acc: 0.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "375it [00:03, 93.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.6079 Acc: 0.9217\n",
      "\n",
      "Training complete in 3m 59s\n",
      "Best Val Acc: 0.9217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
